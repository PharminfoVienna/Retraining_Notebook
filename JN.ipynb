{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSEP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "By importing the file/function using import, Python modules can get access to the code from another module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, rdMolDescriptors\n",
    "from rdkit.Chem import PandasTools as PandasTools\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from standardise import Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 1: Data Upload & Data Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section data provided by the University of Vienna can be combined with own in-house data. The addition of new compounds allows the creation of a new training set that can be used for the retraining of the Machine Learning (ML) models within this Jupyter Notebook (JN). \n",
    "\n",
    "!!! If you don’t want to add additional data, you can skip this step!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### UNIVIE Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "UNIVIE data refers to collected, curated data that has been provided by the University of Vienna for the purpose of training/retraining ML models for six different endpoints:\n",
    "\n",
    "    • BCRP\n",
    "    • BSEP\n",
    "    • OATP1B1\n",
    "    • OATP1B3\n",
    "    • MRP3\n",
    "    • P-gp\n",
    "\n",
    "Datasets that will be used within this JN such as the training set and the test set must be saved within the folder /data.\n",
    "\n",
    "You can select the dataset for the endpoint by changing the file name at the variable “UNIVIE_Data”: e.g.: \"data/BSEP_Univie.sdf\" --> \"data/BCRP_Univie.sdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Please add the name of the UNIVIE file\n",
    "Univie_Data = \"data/BSEP_Univie.sdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Intern Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can include your own in-house dataset for the previous selected endpoint by changing the file name at the variable “Intern_Data” to the name of your dataset.\n",
    "\n",
    "\n",
    "\n",
    "Keep in mind that the path to your dataset has to be properly defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Please add the name of your file\n",
    "Intern_Data = \"data/BSEP_ChEMBL28.sdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load SDF File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The SDF-files are loaded into the JN as dataframes which allow better representation of the information contained within the SDF-files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_Univie_Data = PandasTools.LoadSDF(Univie_Data)\n",
    "df_Intern_Data = PandasTools.LoadSDF(Intern_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### InChIs, SMILES & InChIkey Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To provide additional information about the molecules, the InChIs, SMILES, and InChIkeys are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Intern_Data['InChIs'] = df_Intern_Data['ROMol'].map(lambda x:AllChem.MolToInchi(x))\n",
    "df_Intern_Data['SMILES'] = df_Intern_Data['ROMol'].map(lambda x:AllChem.MolToSmiles(x))\n",
    "df_Intern_Data['InChIKey'] = df_Intern_Data['ROMol'].map(lambda x:AllChem.MolToInchiKey(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove Stereochemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Stereochemistry is removed based on the previously calculated InChIs. This step is performed prior duplicate check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ls_InChIs = df_Intern_Data['InChIs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def removeStereo(fullInchi, position, delimiter):\n",
    "    return delimiter.join(fullInchi.split(delimiter)[:position])\n",
    "\n",
    "ls_Curated_InChIs = []\n",
    "\n",
    "for i in ls_InChIs:\n",
    "    ls_noniso = (removeStereo(i,4,\"/\"))\n",
    "    ls_Curated_InChIs.append(ls_noniso)\n",
    "    \n",
    "df_Intern_Data['Curated_InChIs'] = ls_Curated_InChIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Duplicate Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The duplicates are removed based on the InChIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Intern_Unique = df_Intern_Data.drop_duplicates(subset='Curated_InChIs',keep='first')\n",
    "Intern_Duplicates = df_Intern_Data[df_Intern_Data.duplicated(['Curated_InChIs'], keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Comparison of Classification Values of Removed Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Classification values of duplicates are compared after the duplicate check and compounds with different values are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_classerror(x):\n",
    "        if x in df_classerror:\n",
    "            df_classerror.remove(x)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "if len(Intern_Duplicates.index) == 0:\n",
    "    print('No duplicates found.')\n",
    "else:\n",
    "    Comparison_dupl = Intern_Duplicates[['ROMol','Classification','Curated_InChIs']]\n",
    "    Comparison_uniq = Intern_Unique[['ROMol','Classification','Curated_InChIs']]\n",
    "    merged   =    pd.merge(\n",
    "                  left=Comparison_dupl,\n",
    "                  right=Comparison_uniq,\n",
    "                  how=\"left\",\n",
    "                  left_on=\"Curated_InChIs\",\n",
    "                  right_on=\"Curated_InChIs\")\n",
    "    merged['Class_Match'] = merged.apply(lambda x : str(x.Classification_x) in str(x.Classification_y), axis=1)\n",
    "    Non_Matching_Class = merged.loc[merged['Class_Match'] == False]\n",
    "    df_classerror = Non_Matching_Class['Curated_InChIs'].tolist()\n",
    "    Intern_Unique = Intern_Unique[Intern_Unique.Curated_InChIs.apply(remove_classerror)]\n",
    "    print('Compounds with classification mismatch removed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Duplicate Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "InChIs from the UNIVIE dataset and the Intern dataset are compared with each other and duplicates are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(x):\n",
    "    if x in ls_Univie_Transporters:\n",
    "        ls_Univie_Transporters.remove(x)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "ls_Univie_Transporters = df_Univie_Data['Curated_InChIs'].tolist()\n",
    "\n",
    "\n",
    "Delta_Compounds = Intern_Unique[Intern_Unique.Curated_InChIs.apply(remove_duplicates)]\n",
    "Delta_Compounds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Selection of Important Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The columns: Classification, ROMol, InChI, SMILES and InChIKey are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Columns_Univie =  df_Univie_Data[['Classification', 'ROMol', 'Curated_InChIs','SMILES','InChIKey']]\n",
    "Columns_Intern = Delta_Compounds[['Classification', 'ROMol', 'Curated_InChIs','SMILES','InChIKey']]\n",
    "Training_Set = pd.concat([Columns_Univie, Columns_Intern], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Save Generated Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A SDF-file is generated namely:” Training_Set.sdf” which includes a new training set with the above mentioned columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(Training_Set,\"data/BSEP_Training_Set.sdf\",properties=list(Training_Set.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 2: Data Set Preparation for ML Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read SDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The training set is uploaded into the JN at the variable \"molecules\" via the RDKit functionality for working with molecular file format.\n",
    "\n",
    "You can select the training set for the endpoint by changing the file name at the variable \"molecules\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Please add the name of your training set\n",
    "molecules = Chem.ForwardSDMolSupplier(\"data/BSEP_Univie.sdf\", sanitize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The test set is uploaded into the JN at the variable \"test_molecules\" via the RDKit functionality for working with molecular file format.\n",
    "\n",
    "You can select the test set for the endpoint by changing the file name at the variable \"test_molecules\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Please add the name of your test set\n",
    "test_molecules = Chem.ForwardSDMolSupplier(\"data/BSEP_ChEMBL28.sdf\", sanitize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compounds within the training set are standardised using a modified atkinson standardizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stand = Standardization()\n",
    "\n",
    "train_molcount = 0\n",
    "\n",
    "standardised_molecules = []\n",
    "            \n",
    "for mol in molecules:\n",
    "        train_molcount += 1\n",
    "\n",
    "        if mol is None:\n",
    "            continue\n",
    "\n",
    "        standardisation_ok, molOrError = stand.standardise(mol)\n",
    "        \n",
    "        if standardisation_ok == True:\n",
    "            standardised_molecules.append(molOrError)\n",
    "        else:\n",
    "            print(molOrError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A standardised training set dataframe is created that can be saved as a SDF-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_standardised_train_molecules = pd.DataFrame(standardised_molecules)\n",
    "df_standardised_train_molecules.rename(columns = {0:'ROMol'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A SDF-file is generated namely:” standardised_train_molecules.sdf” which includes the standardised training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_standardised_train_molecules,\"data/BSEP_standardised_train_molecules.sdf\",properties=list(df_standardised_train_molecules.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compounds within the test set are standardised using a modified atkinson standardizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stand = Standardization()\n",
    "\n",
    "test_molcount = 0\n",
    "\n",
    "standardised_test_molecules = []\n",
    "\n",
    "for mol in test_molecules:\n",
    "        test_molcount += 1\n",
    "        \n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        standardisation_ok, molOrError = stand.standardise(mol)\n",
    "        \n",
    "        if standardisation_ok == True:\n",
    "            standardised_test_molecules.append(molOrError)\n",
    "        else:\n",
    "            print(molOrError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A standardised test set dataframe is created that can be saved as a SDF-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_standardised_test_molecules = pd.DataFrame(standardised_test_molecules)\n",
    "df_standardised_test_molecules.rename(columns = {0:'ROMol'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A SDF-file is generated namely:” standardised_test_molecules.sdf” which includes the standardised test set.\n",
    "\n",
    "This standardised test set is later used for the applicability domain (AD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_standardised_test_molecules,\"data/BSEP_standardised_test_molecules.sdf\",properties=list(df_standardised_test_molecules.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The number of compounds within the training set that were able to be standardised is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (str(len(standardised_molecules)) + ' of ' + str(train_molcount) + ' compounds could be standardised.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The number of compounds within the test set that were able to be standardised is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (str(len(standardised_test_molecules)) + ' of ' + str(test_molcount) + ' compounds could be standardised.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The 70 predefined RDKit descriptors are available in the file \"RDKIT_Descriptors.txt\".\n",
    "\n",
    "These descriptors have been found to be important for describing the six enpdoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"RDKIT_Descriptors.txt\", \"r\") as f:\n",
    "    Descriptors = []\n",
    "    for descriptor in f.readlines():\n",
    "        items = descriptor.rstrip('\\n').rstrip(',').strip(\"''\")\n",
    "        Descriptors.append(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Checks if the 70 descriptors are used and the number of descriptors used is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (str(len(Descriptors)) + ' descriptors are used.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "70 descriptors are calculated for each compound within the standardised training set. \n",
    "\n",
    "The results and the classification values are saved as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calculator = MoleculeDescriptors.MolecularDescriptorCalculator(Descriptors)\n",
    "descriptors = []\n",
    "activities = []\n",
    "\n",
    "for mol in standardised_molecules:\n",
    "\n",
    "    desc_np = np.asarray(calculator.CalcDescriptors(mol))\n",
    "\n",
    "    descriptors.append(desc_np)\n",
    "\n",
    "    activities.append(int(mol.GetProp(\"Classification\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "70 descriptors are calculated for each compound within the standardised test set. \n",
    "\n",
    "The results and the classification values are saved as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calculator_test = MoleculeDescriptors.MolecularDescriptorCalculator(Descriptors)\n",
    "descriptors_test = []\n",
    "activities_test = []\n",
    "\n",
    "for mol in standardised_test_molecules:\n",
    "    desc_np = np.asarray(calculator_test.CalcDescriptors(mol))\n",
    "    descriptors_test.append(desc_np)\n",
    "    activities_test.append(int(mol.GetProp(\"Classification\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check Descriptor Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results of the descriptor calculation and the classification values are displayed for the standardised training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The descriptors for ' + str(len(descriptors)) + ' compounds could be calculated.')\n",
    "print('The activities of ' + str(len(activities)) + ' compounds could be read.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results of the descriptor calculation and the classification values are displayed for the standardised test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The descriptors for ' + str(len(descriptors_test)) + ' compounds could be calculated.')\n",
    "print('The activities of ' + str(len(activities_test)) + ' compounds could be read.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Count actives / inactives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The number of inhibitors, non-inhibitors as well as the total amount of compounds within the standardised training set are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "active = 0\n",
    "inactive = 0\n",
    "for activity in activities:\n",
    "    if activity == 1:\n",
    "        active += 1\n",
    "    elif activity == 0:\n",
    "        inactive += 1\n",
    "print('The training set includes', active, 'inhibitors and', inactive, 'non-inhibitors.')\n",
    "print('The total amount of compounds in the training set is', active + inactive, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The number of inhibitors, non-inhibitors as well as the total amount of compounds within the standardised test set are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "active_test = 0\n",
    "inactive_test = 0\n",
    "for activity_test in activities_test:\n",
    "    if activity_test == 1:\n",
    "        active_test += 1\n",
    "    elif activity_test == 0:\n",
    "        inactive_test += 1\n",
    "print('The training set includes', active_test, 'inhibitors and', inactive_test, 'non-inhibitors.')\n",
    "print('The total amount of compounds in the training set is', active_test + inactive_test, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Change NaN to 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NaN values are replaced with zeros for avoiding potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_descriptors = pd.DataFrame(descriptors)\n",
    "zeros = df_descriptors.fillna(0.0)\n",
    "descriptors = np.array(zeros)\n",
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Applicability Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local outlier factor (LOF) as described by Sahigara and coworkers is used for the calculation of the applicability domain (Sahigara F, Ballabio D, Todeschini R, Consonni V (2013) Defining a novel k-nearest neighbours approach to assess the applicability domain of a QSAR model for reliable predictions. J Cheminform 5:27. https://doi.org/10.1186/1758-2946-5-27) and was implemented in the scikit-learn Python library (version 0.24.2). \n",
    "\n",
    "In this approach the local densities of the nearest neighbors of a compound are compared to its local densities, and a factor from 0 to 1 is assigned. In brief, if the local density is greater or equal to its surrounding, a compound is considered inside the domain, otherwise it is considered outside the domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that the test set is within the chemical space of the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select the test set for the AD by changing the file name at the variable “Test_Data\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Data = \"data/BSEP_ChEMBL28.sdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AD is calculated and displayed as a depiction for a general representation if the test set is inside or outside of the AD. \n",
    "\n",
    "Red line = Applicability domain\n",
    "\n",
    "White dots = Test set compounds\n",
    "\n",
    "Blue colour = Frontier-delimited subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "iso_map = PCA(n_components=2)\n",
    "model = LocalOutlierFactor(n_neighbors=5, novelty=True, contamination=0.1, metric='euclidean')\n",
    "\n",
    "fps = []\n",
    "thresholds = []\n",
    "k = 15\n",
    "stand = Standardization()\n",
    "\n",
    "scaler.fit(descriptors)\n",
    "stand_desc = scaler.transform(descriptors)\n",
    "iso_map.fit(stand_desc)\n",
    "train_iso = iso_map.transform(stand_desc)\n",
    "model.fit(train_iso)\n",
    "\n",
    "tests = []\n",
    "test_stand_desc = scaler.transform(descriptors_test)\n",
    "test_iso = iso_map.transform(test_stand_desc)\n",
    "#tests.append([test_iso[0][0], test_iso[0][1]])\n",
    "iso_ad_pred = []\n",
    "for tp in test_iso:\n",
    "    iso_ad_pred.append(model.predict([tp])[0])\n",
    "    tests.append([tp[0], tp[1]])\n",
    "\n",
    "\n",
    "tests = np.array(tests)\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Applicability Domain Assessment with LOF\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "s = 40\n",
    "b2 = plt.scatter(tests[:, 0], tests[:, 1], c='white', s=s,edgecolors='k', zorder=3)\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0],b2],\n",
    "  [\"Applicability Domain\",\n",
    "   \"Test Set Components\"],\n",
    "  loc=\"upper left\",\n",
    "  prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.ylabel(\"PCA Dimension 1\")\n",
    "plt.xlabel(\"PCA Dimension 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is checked if it is within the chemical space of the model and an additional column named \"LOF\" is created containing \"1\" for inlier and \"-1\" for outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# if lof equal to 1, then it is considered as an inlier; if it is -1, then it is an outlier.\n",
    "df_descriptors_test = pd.DataFrame(descriptors_test, columns = Descriptors)\n",
    "df_descriptors_test['LOF'] = iso_ad_pred\n",
    "Test_Outliers = df_descriptors_test[df_descriptors_test['LOF'] == -1]\n",
    "New_Descriptors_test = df_descriptors_test[df_descriptors_test['LOF'] == 1]\n",
    "\n",
    "# Read data\n",
    "df_Test_Data = PandasTools.LoadSDF(Test_Data)\n",
    "\n",
    "# Search via index outliers \n",
    "Outlier_Compounds = df_Test_Data['ROMol'].loc[Test_Outliers.index]\n",
    "df_Outlier_Compounds = pd.DataFrame(Outlier_Compounds)\n",
    "print(str(len(Test_Outliers)) + ' compounds have been identified as outliers in the Test Set using LOF.')\n",
    "\n",
    "for index, row in df_Outlier_Compounds.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A SDF-file is generated namely:” Outlier_Compounds.sdf” which includes all compounds that are out of domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_Outlier_Compounds,\"results/BSEP_Outlier_Compounds.sdf\",properties=list(df_Outlier_Compounds.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model Generation & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four different classifiers are used for model generation:\n",
    "\n",
    "    • Logistic Regression (LR)\n",
    "    • Support Vector Machine (SVM)\n",
    "    • Random Forest (RF)\n",
    "    • k-nearest neighbor (KNN)\n",
    "\n",
    "The scikit-learn Python library (version 0.24.2) implementations are used to train binary classification models for the six mentioned endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please add the name of the used training set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select the dataset for the creation of the ML models by changing the file name at the variable “Train_Stand_SDF”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train_Stand_SDF = \"data/BSEP_standardised_train_molecules.sdf\"\n",
    "df_Data = PandasTools.LoadSDF(Train_Stand_SDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please add the name of the used test set*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can select the dataset for testing the ML models by changing the file name at the variable “Test_Stand_SDF”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Stand_SDF = \"data/BSEP_standardised_test_molecules.sdf\"\n",
    "df_Test_Data = PandasTools.LoadSDF(Test_Stand_SDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Logisitic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LR model is created with pre-selected hyperparameters and fitted on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(solver='saga', max_iter=10000)\n",
    "lr_clf.fit(descriptors,activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pickle-file is generated namely:” model_LR.pkl” which includes the model. \n",
    "\n",
    "Pickle in Python is used in serializing and deserializing a Python object structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_LR.pkl'\n",
    "pickle.dump(lr_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR -  10-CV Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the models is evaluated using a 10-fold cross-validation, and following statistical metrics are used:\n",
    "\n",
    "    • Accuracy Score\n",
    "    • Sensitivity Score\n",
    "    • Specificity Score\n",
    "    • Balanced Accuracy (BA) Score\n",
    "    • F1 Score\n",
    "    • Area Under the Curve (AUC) Score\n",
    "    • Precision Score\n",
    "    • Matthews Correlation Coefficient (MCC) Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall_score = cross_val_score(ANOVA_svm_model, descriptors_fs, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "f1_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(lr_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "matthews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring=matthews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lr1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_lr1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_lr1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_lr1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_lr1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_lr1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_lr1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_lr1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dropdown field is generated, and the statistical metric results of interest can be displayed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_lr1], ['Sensitivity',sen_lr1], ['Specificity',spec_lr1], ['Balanced Accuracy',ba_lr1],['F1_score',f1_lr1], ['AUC',auc_lr1], ['Precision',prec_lr1], ['MCC',mcc_lr1]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrectly predicted compounds are indentified and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "log_predictions = lr_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != log_predictions[i]]\n",
    "wrong_predictions_lr = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_lr.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted incorrectly.')\n",
    "compounds_lr_wrong =  pd.DataFrame(wrong_predictions_lr['ROMol'])\n",
    "\n",
    "for index, row in compounds_lr_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A SDF-file is generated namely:” WP_LR_Compounds.sdf” which includes all compounds that are predicted incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_lr,\"data/BSEP_WP_LR_Compounds.sdf\",properties=list(wrong_predictions_lr.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression -  Evaluation Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LR model is used for the prediction of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predict = lr_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the models is tested , and following statistical metrics are used:\n",
    "\n",
    "    • Accuracy Score\n",
    "    • Sensitivity Score\n",
    "    • Specificity Score\n",
    "    • Balanced Accuracy (BA) Score\n",
    "    • F1 Score\n",
    "    • Area Under the Curve (AUC) Score\n",
    "    • Precision Score\n",
    "    • Matthews Correlation Coefficient (MCC) Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lr2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), lr_predict)\n",
    "sen_lr2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), lr_predict)\n",
    "spec_lr2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), lr_predict, pos_label=0)\n",
    "ba_lr2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), lr_predict)\n",
    "f1_lr2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), lr_predict)\n",
    "auc_lr2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), lr_predict))\n",
    "prec_lr2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), lr_predict)\n",
    "mcc_lr2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), lr_predict)\n",
    "rec_lr2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), lr_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dropdown field is generated, and the statistical metric results of interest can be displayed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_lr2], ['Sensitivity',sen_lr2], ['Specificity',spec_lr2], ['Balanced Accuracy',ba_lr2],['F1_score',f1_lr2], ['AUC',auc_lr2], ['Precision',prec_lr2], ['MCC',mcc_lr2], ['Recall',rec_lr2]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulted prediction is compared with the classification values of the test set and an additional column named \"Prediction Comparison\" is created. This column contains \"Right\" if the prediction matches the classification value or \"Wrong\" if not.\n",
    "\n",
    "In addition a column named \"Domain\" is created. This column indicates if the compounds within the test set are within the AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test = df_Test_Data\n",
    "df_Test[\"Prediction\"] = lr_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_LR_Test = df_Test\n",
    "print(df_LR_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SDF-file is generated namely:” LR_Test_Prediction.sdf” which includes the information above mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_LR_Test,\"results/BSEP_LR_Test_Prediction.sdf\",properties=list(df_LR_Test.columns))\n",
    "df_LR_Test.to_csv(\"results/BSEP_LR_Test_Prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SVM model is created with pre-selected hyperparameters and fitted on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC(class_weight='balanced', C= 1.0, kernel= 'linear')\n",
    "svm_clf.fit(descriptors,activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pickle-file is generated namely:” model_SVM.pkl” which includes the model. \n",
    "\n",
    "Pickle in Python is used in serializing and deserializing a Python object structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_SVM.pkl'\n",
    "pickle.dump(svm_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM  -  10-CV Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the models is evaluated using a 10-fold cross-validation, and following statistical metrics are used:\n",
    "\n",
    "    • Accuracy Score\n",
    "    • Sensitivity Score\n",
    "    • Specificity Score\n",
    "    • Balanced Accuracy (BA) Score\n",
    "    • F1 Score\n",
    "    • Area Under the Curve (AUC) Score\n",
    "    • Precision Score\n",
    "    • Matthews Correlation Coefficient (MCC) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(svm_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "matthews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring=matthews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_svm1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_svm1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_svm1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_svm1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_svm1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_svm1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_svm1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_svm1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dropdown field is generated, and the statistical metric results of interest can be displayed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_svm1], ['Sensitivity',sen_svm1], ['Specificity',spec_svm1], ['Balanced Accuracy',ba_svm1],['F1_score',f1_svm1], ['AUC',auc_svm1], ['Precision',prec_svm1], ['MCC',mcc_svm1]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrectly predicted compounds are indentified and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "svm_predictions = svm_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != svm_predictions[i]]\n",
    "wrong_predictions_svm = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_svm.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted incorrectly.')\n",
    "compounds_svm_wrong =  pd.DataFrame(wrong_predictions_svm['ROMol'])\n",
    "\n",
    "for index, row in compounds_svm_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A SDF-file is generated namely:” WP_SVM_Compounds.sdf” which includes all compounds that are predicted incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_svm,\"results/BSEP_WP_SVM_Compounds.sdf\",properties=list(wrong_predictions_svm.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SVM -  Evaluation Test  Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM model is used for the prediction of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predict = svm_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_svm2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), svm_predict)\n",
    "sen_svm2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), svm_predict)\n",
    "spec_svm2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), svm_predict, pos_label=0)\n",
    "ba_svm2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), svm_predict)\n",
    "f1_svm2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), svm_predict)\n",
    "auc_svm2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), svm_predict))\n",
    "prec_svm2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), svm_predict)\n",
    "mcc_svm2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), svm_predict)\n",
    "rec_svm2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), svm_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the models is tested , and following statistical metrics are used:\n",
    "\n",
    "    • Accuracy Score\n",
    "    • Sensitivity Score\n",
    "    • Specificity Score\n",
    "    • Balanced Accuracy (BA) Score\n",
    "    • F1 Score\n",
    "    • Area Under the Curve (AUC) Score\n",
    "    • Precision Score\n",
    "    • Matthews Correlation Coefficient (MCC) Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_svm2], ['Sensitivity',sen_svm2], ['Specificity',spec_svm2], ['Balanced Accuracy',ba_svm2],['F1_score',f1_svm2], ['AUC',auc_svm2], ['Precision',prec_svm2], ['MCC',mcc_svm2],['Recall',rec_svm2]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulted prediction is compared with the classification values of the test set and an additional column named \"Prediction Comparison\" is created. This column contains \"Right\" if the prediction matches the classification value or \"Wrong\" if not.\n",
    "\n",
    "In addition a column named \"Domain\" is created. This column indicates if the compounds within the test set are within the AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test = df_Test_Data\n",
    "df_Test[\"Prediction\"] = svm_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_SVM_Test = df_Test\n",
    "print(df_SVM_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SDF-file is generated namely:” SVM_Test_Prediction.sdf” which includes the information above mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_SVM_Test,\"results/BSEP_SVM_Test_Prediction.sdf\",properties=list(df_SVM_Test.columns))\n",
    "df_SVM_Test.to_csv(\"results/BSEP_SVM_Test_Prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RF model is created with pre-selected hyperparameters and fitted on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(class_weight=\"balanced\", max_depth = 4, n_estimators = 10)\n",
    "rf_clf.fit(descriptors, activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pickle-file is generated namely:” model_RF.pkl” which includes the model. \n",
    "\n",
    "Pickle in Python is used in serializing and deserializing a Python object structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_RF.pkl'\n",
    "pickle.dump(rf_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF - 10-CV Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the models is evaluated using a 10-fold cross-validation, and following statistical metrics are used:\n",
    "\n",
    "    • Accuracy Score\n",
    "    • Sensitivity Score\n",
    "    • Specificity Score\n",
    "    • Balanced Accuracy (BA) Score\n",
    "    • F1 Score\n",
    "    • Area Under the Curve (AUC) Score\n",
    "    • Precision Score\n",
    "    • Matthews Correlation Coefficient (MCC) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(rf_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "matthews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring=matthews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rf1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_rf1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_rf1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_rf1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_rf1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_rf1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_rf1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_rf1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dropdown field is generated, and the statistical metric results of interest can be displayed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_rf1], ['Sensitivity',sen_rf1], ['Specificity',spec_rf1], ['Balanced Accuracy',ba_rf1],['F1_score',f1_rf1], ['AUC',auc_rf1], ['Precision',prec_rf1], ['MCC',mcc_rf1]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrectly predicted compounds are indentified and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "rf_predictions = rf_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != rf_predictions[i]]\n",
    "wrong_predictions_rf = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_rf.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted incorrectly.')\n",
    "compounds_rf_wrong =  pd.DataFrame(wrong_predictions_rf['ROMol'])\n",
    "\n",
    "for index, row in compounds_rf_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A SDF-file is generated namely:” WP_RF_Compounds.sdf” which includes all compounds that are predicted incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_rf,\"results/BSEP_WP_RF_Compounds.sdf\",properties=list(wrong_predictions_rf.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF - Evaluation Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RF model is used for the prediction of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predict = rf_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the models is tested , and following statistical metrics are used:\n",
    "\n",
    "    • Accuracy Score\n",
    "    • Sensitivity Score\n",
    "    • Specificity Score\n",
    "    • Balanced Accuracy (BA) Score\n",
    "    • F1 Score\n",
    "    • Area Under the Curve (AUC) Score\n",
    "    • Precision Score\n",
    "    • Matthews Correlation Coefficient (MCC) Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rf2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), rf_predict)\n",
    "sen_rf2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), rf_predict)\n",
    "spec_rf2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), rf_predict, pos_label=0)\n",
    "ba_rf2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), rf_predict)\n",
    "f1_rf2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), rf_predict)\n",
    "auc_rf2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), rf_predict))\n",
    "prec_rf2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), rf_predict)\n",
    "mcc_rf2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), rf_predict)\n",
    "rec_rf2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), rf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dropdown field is generated, and the statistical metric results of interest can be displayed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_rf2], ['Sensitivity',sen_rf2], ['Specificity',spec_rf2], ['Balanced Accuracy',ba_rf2],['F1_score',f1_rf2], ['AUC',auc_rf2], ['Precision',prec_rf2], ['MCC',mcc_rf2],['Recall',rec_rf2]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulted prediction is compared with the classification values of the test set and an additional column named \"Prediction Comparison\" is created. This column contains \"Right\" if the prediction matches the classification value or \"Wrong\" if not.\n",
    "\n",
    "In addition a column named \"Domain\" is created. This column indicates if the compounds within the test set are within the AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test = df_Test_Data\n",
    "df_Test[\"Prediction\"] = rf_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_RF_Test = df_Test\n",
    "print(df_RF_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SDF-file is generated namely:” RF_Test_Prediction.sdf” which includes the information above mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_RF_Test,\"results/BSEP_RF_Test_Prediction.sdf\",properties=list(df_RF_Test.columns))\n",
    "df_RF_Test.to_csv(\"results/BSEP_RF_Test_Prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A KNN model is created with pre-selected hyperparameters and fitted on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(metric =  'euclidean', n_neighbors = 3, weights =  'distance')\n",
    "knn_clf.fit(descriptors,activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pickle-file is generated namely:” model_KNN.pkl” which includes the model. \n",
    "\n",
    "Pickle in Python is used in serializing and deserializing a Python object structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_KNN.pkl'\n",
    "pickle.dump(knn_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  KNN -  10-CV Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the models is evaluated using a 10-fold cross-validation, and following statistical metrics are used:\n",
    "\n",
    "    • Accuracy Score\n",
    "    • Sensitivity Score\n",
    "    • Specificity Score\n",
    "    • Balanced Accuracy (BA) Score\n",
    "    • F1 Score\n",
    "    • Area Under the Curve (AUC) Score\n",
    "    • Precision Score\n",
    "    • Matthews Correlation Coefficient (MCC) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(knn_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "matthews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring=matthews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_knn1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_knn1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_knn1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_knn1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_knn1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_knn1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_knn1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_knn1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dropdown field is generated, and the statistical metric results of interest can be displayed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_knn1], ['Sensitivity',sen_knn1], ['Specificity',spec_knn1], ['Balanced Accuracy',ba_knn1],['F1_score',f1_knn1], ['AUC',auc_knn1], ['Precision',prec_knn1], ['MCC',mcc_knn1]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorrectly predicted compounds are indentified and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "knn_predictions = knn_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != knn_predictions[i]]\n",
    "wrong_predictions_knn = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_knn.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted incorrectly.')\n",
    "compounds_knn_wrong =  pd.DataFrame(wrong_predictions_knn['ROMol'])\n",
    "\n",
    "for index, row in compounds_knn_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Incorrectly Predicted Compounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A SDF-file is generated namely:” WP_KNN_Compounds.sdf” which includes all compounds that are predicted incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_knn,\"results/BSEP_WP_KNN_Compounds.sdf\",properties=list(wrong_predictions_knn.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  KNN -  Evaluation Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN model is used for the prediction of the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predict = knn_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_knn2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), knn_predict)\n",
    "sen_knn2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), knn_predict)\n",
    "spec_knn2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), knn_predict, pos_label=0)\n",
    "ba_knn2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), knn_predict)\n",
    "f1_knn2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), knn_predict)\n",
    "auc_knn2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), knn_predict))\n",
    "prec_knn2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), knn_predict)\n",
    "mcc_knn2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), knn_predict)\n",
    "rec_knn2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), knn_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dropdown field is generated, and the statistical metric results of interest can be displayed one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_knn2], ['Sensitivity',sen_knn2], ['Specificity',spec_knn2], ['Balanced Accuracy',ba_knn2],['F1_score',f1_knn2], ['AUC',auc_knn2], ['Precision',prec_knn2], ['MCC',mcc_knn2],['Recall',rec_knn2]],\n",
    "    description= 'Choose one:',\n",
    "    value = None,\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulted prediction is compared with the classification values of the test set and an additional column named \"Prediction Comparison\" is created. This column contains \"Right\" if the prediction matches the classification value or \"Wrong\" if not.\n",
    "\n",
    "In addition a column named \"Domain\" is created. This column indicates if the compounds within the test set are within the AD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test = df_Test_Data\n",
    "df_Test[\"Prediction\"] = knn_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_KNN_Test = df_Test\n",
    "print(df_KNN_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SDF-file is generated namely:” KNN_Test_Prediction.sdf” which includes the information above mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_KNN_Test,\"results/BSEP_KNN_Test_Prediction.sdf\",properties=list(df_KNN_Test.columns))\n",
    "df_KNN_Test.to_csv(\"results/BSEP_KNN_Test_Prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:retraining_env] *",
   "language": "python",
   "name": "conda-env-retraining_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "410.66px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
