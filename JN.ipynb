{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSEP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, rdMolDescriptors\n",
    "from rdkit.Chem import PandasTools as PandasTools\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import preprocessing\n",
    "from standardise import Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIVIE Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Univie_Data = \"data/BSEP_Univie.sdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intern Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please add the name of your file\n",
    "Intern_Data = \"data/BSEP_ChEMBL28.sdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Univie_Data = PandasTools.LoadSDF(Univie_Data)\n",
    "df_Intern_Data = PandasTools.LoadSDF(Intern_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InChIs, SMILES & InChIkey Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_Intern_Data['InChIs'] = df_Intern_Data['ROMol'].map(lambda x:AllChem.MolToInchi(x))\n",
    "df_Intern_Data['SMILES'] = df_Intern_Data['ROMol'].map(lambda x:AllChem.MolToSmiles(x))\n",
    "df_Intern_Data['InChIKey'] = df_Intern_Data['ROMol'].map(lambda x:AllChem.MolToInchiKey(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stereochemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_InChIs = df_Intern_Data['InChIs'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def removeStereo(fullInchi, position, delimiter):\n",
    "    return delimiter.join(fullInchi.split(delimiter)[:position])\n",
    "\n",
    "ls_Curated_InChIs = []\n",
    "\n",
    "for i in ls_InChIs:\n",
    "    ls_noniso = (removeStereo(i,4,\"/\"))\n",
    "    ls_Curated_InChIs.append(ls_noniso)\n",
    "    \n",
    "df_Intern_Data['Curated_InChIs'] = ls_Curated_InChIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intern_Unique = df_Intern_Data.drop_duplicates(subset='Curated_InChIs',keep='first')\n",
    "Intern_Duplicates = df_Intern_Data[df_Intern_Data.duplicated(['Curated_InChIs'], keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Classification Values of Removed Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_classerror(x):\n",
    "        if x in df_classerror:\n",
    "            df_classerror.remove(x)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "if len(Intern_Duplicates.index) == 0:\n",
    "    print('No duplicates found.')\n",
    "else:\n",
    "    Comparison_dupl = Intern_Duplicates[['ROMol','Classification','Curated_InChIs']]\n",
    "    Comparison_uniq = Intern_Unique[['ROMol','Classification','Curated_InChIs']]\n",
    "    merged   =    pd.merge(\n",
    "                  left=Comparison_dupl,\n",
    "                  right=Comparison_uniq,\n",
    "                  how=\"left\",\n",
    "                  left_on=\"Curated_InChIs\",\n",
    "                  right_on=\"Curated_InChIs\")\n",
    "    merged['Class_Match'] = merged.apply(lambda x : str(x.Classification_x) in str(x.Classification_y), axis=1)\n",
    "    Non_Matching_Class = merged.loc[merged['Class_Match'] == False]\n",
    "    df_classerror = Non_Matching_Class['Curated_InChIs'].tolist()\n",
    "    Intern_Unique = Intern_Unique[Intern_Unique.Curated_InChIs.apply(remove_classerror)]\n",
    "    print('Compounds with classification mismatch removed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(x):\n",
    "    if x in ls_Univie_Transporters:\n",
    "        ls_Univie_Transporters.remove(x)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "ls_Univie_Transporters = df_Univie_Data['Curated_InChIs'].tolist()\n",
    "\n",
    "\n",
    "Delta_Compounds = Intern_Unique[Intern_Unique.Curated_InChIs.apply(remove_duplicates)]\n",
    "Delta_Compounds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Important Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Columns_Univie =  df_Univie_Data[['Classification', 'ROMol', 'Curated_InChIs','SMILES','InChIKey']]\n",
    "Columns_Intern = Delta_Compounds[['Classification', 'ROMol', 'Curated_InChIs','SMILES','InChIKey']]\n",
    "Training_Set = pd.concat([Columns_Univie, Columns_Intern], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Generated Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(Training_Set,\"data/BSEP_Training_Set.sdf\",properties=list(Training_Set.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 2: Data Set Preparation for ML Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Read SDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Please add the name of your training set\n",
    "molecules = Chem.ForwardSDMolSupplier(\"data/BSEP_Univie.sdf\", sanitize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Please add the name of your test set\n",
    "test_molecules = Chem.ForwardSDMolSupplier(\"data/BSEP_ChEMBL28.sdf\", sanitize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stand = Standardization()\n",
    "\n",
    "train_molcount = 0\n",
    "\n",
    "standardised_molecules = []\n",
    "            \n",
    "for mol in molecules:\n",
    "        train_molcount += 1\n",
    "\n",
    "        if mol is None:\n",
    "            continue\n",
    "\n",
    "        standardisation_ok, molOrError = stand.standardise(mol)\n",
    "        \n",
    "        if standardisation_ok == True:\n",
    "            standardised_molecules.append(molOrError)\n",
    "        else:\n",
    "            print(molOrError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stand = Standardization()\n",
    "\n",
    "test_molcount = 0\n",
    "\n",
    "standardised_test_molecules = []\n",
    "\n",
    "for mol in test_molecules:\n",
    "        test_molcount += 1\n",
    "        \n",
    "        if mol is None:\n",
    "            continue\n",
    "            \n",
    "        standardisation_ok, molOrError = stand.standardise(mol)\n",
    "        \n",
    "        if standardisation_ok == True:\n",
    "            standardised_test_molecules.append(molOrError)\n",
    "        else:\n",
    "            print(molOrError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Check Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (str(len(standardised_molecules)) + ' of ' + str(train_molcount) + ' compounds could be standardised.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (str(len(standardised_test_molecules)) + ' of ' + str(test_molcount) + ' compounds could be standardised.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"RDKIT_Descriptors.txt\", \"r\") as f:\n",
    "    Descriptors = []\n",
    "    for descriptor in f.readlines():\n",
    "        items = descriptor.rstrip('\\n').rstrip(',').strip(\"''\")\n",
    "        Descriptors.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (str(len(Descriptors)) + ' descriptors are used.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calculator = MoleculeDescriptors.MolecularDescriptorCalculator(Descriptors)\n",
    "descriptors = []\n",
    "activities = []\n",
    "\n",
    "for mol in standardised_molecules:\n",
    "\n",
    "    desc_np = np.asarray(calculator.CalcDescriptors(mol))\n",
    "\n",
    "    descriptors.append(desc_np)\n",
    "\n",
    "    activities.append(int(mol.GetProp(\"Classification\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calculator_test = MoleculeDescriptors.MolecularDescriptorCalculator(Descriptors)\n",
    "descriptors_test = []\n",
    "activities_test = []\n",
    "\n",
    "for mol in standardised_test_molecules:\n",
    "    desc_np = np.asarray(calculator_test.CalcDescriptors(mol))\n",
    "    descriptors_test.append(desc_np)\n",
    "    activities_test.append(int(mol.GetProp(\"Classification\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check Descriptor Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The descriptors for ' + str(len(descriptors)) + ' compounds could be calculated.')\n",
    "print('The activities of ' + str(len(activities)) + ' compounds could be read.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('The descriptors for ' + str(len(descriptors_test)) + ' compounds could be calculated.')\n",
    "print('The activities of ' + str(len(activities_test)) + ' compounds could be read.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Count actives / inactives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "active = 0\n",
    "inactive = 0\n",
    "for activity in activities:\n",
    "    if activity == 1:\n",
    "        active += 1\n",
    "    elif activity == 0:\n",
    "        inactive += 1\n",
    "print('The training set includes', active, 'inhibitors and', inactive, 'non-inhibitors.')\n",
    "print('The total amount of compounds in the training set is', active + inactive, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "active_test = 0\n",
    "inactive_test = 0\n",
    "for activity_test in activities_test:\n",
    "    if activity_test == 1:\n",
    "        active_test += 1\n",
    "    elif activity_test == 0:\n",
    "        inactive_test += 1\n",
    "print('The training set includes', active_test, 'inhibitors and', inactive_test, 'non-inhibitors.')\n",
    "print('The total amount of compounds in the training set is', active_test + inactive_test, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Change NaN to 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_descriptors = pd.DataFrame(descriptors)\n",
    "zeros = df_descriptors.fillna(0.0)\n",
    "descriptors = np.array(zeros)\n",
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 3: Applicability Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Add your test data for comparison*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Test_Data = \"data/BSEP_ChEMBL28.sdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "iso_map = PCA(n_components=2)\n",
    "model = LocalOutlierFactor(n_neighbors=5, novelty=True, contamination=0.1, metric='euclidean')\n",
    "\n",
    "fps = []\n",
    "thresholds = []\n",
    "k = 15\n",
    "stand = Standardization()\n",
    "\n",
    "scaler.fit(descriptors)\n",
    "stand_desc = scaler.transform(descriptors)\n",
    "iso_map.fit(stand_desc)\n",
    "train_iso = iso_map.transform(stand_desc)\n",
    "model.fit(train_iso)\n",
    "\n",
    "tests = []\n",
    "test_stand_desc = scaler.transform(descriptors_test)\n",
    "test_iso = iso_map.transform(test_stand_desc)\n",
    "#tests.append([test_iso[0][0], test_iso[0][1]])\n",
    "iso_ad_pred = []\n",
    "for tp in test_iso:\n",
    "    iso_ad_pred.append(model.predict([tp])[0])\n",
    "    tests.append([tp[0], tp[1]])\n",
    "\n",
    "\n",
    "tests = np.array(tests)\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Applicability Domain Assesment with LOF\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "s = 40\n",
    "b2 = plt.scatter(tests[:, 0], tests[:, 1], c='white', s=s,edgecolors='k', zorder=3)\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0],b2],\n",
    "  [\"Applicability Domain\",\n",
    "   \"Test Set Components\"],\n",
    "  loc=\"upper left\",\n",
    "  prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.ylabel(\"PCA Dimension 1\")\n",
    "plt.xlabel(\"PCA Dimension 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# if lof equal to 1, then it is considered as an inlier; if it is -1, then it is an outlier.\n",
    "df_descriptors_test = pd.DataFrame(descriptors_test, columns = Descriptors)\n",
    "df_descriptors_test['LOF'] = iso_ad_pred\n",
    "Test_Outliers = df_descriptors_test[df_descriptors_test['LOF'] == -1]\n",
    "New_Descriptors_test = df_descriptors_test[df_descriptors_test['LOF'] == 1]\n",
    "\n",
    "# Read data\n",
    "df_Test_Data = PandasTools.LoadSDF(Test_Data)\n",
    "\n",
    "# Search via index outliers \n",
    "Outlier_Compounds = df_Test_Data['ROMol'].loc[Test_Outliers.index]\n",
    "df_Outlier_Compounds = pd.DataFrame(Outlier_Compounds)\n",
    "print(str(len(Test_Outliers)) + ' compounds have been identified as outliers in the Test Set using LOF.')\n",
    "\n",
    "for index, row in df_Outlier_Compounds.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_Outlier_Compounds,\"results/BSEP_Outlier_Compounds.sdf\",properties=list(df_Outlier_Compounds.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 4: Model Generation & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Please add the name of the used training set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SDFFile = \"data/BSEP_Univie.sdf\"\n",
    "df_Data = PandasTools.LoadSDF(SDFFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1) Logisitic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(solver='saga', max_iter=10000)\n",
    "lr_clf.fit(descriptors,activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_LR.pkl'\n",
    "pickle.dump(lr_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### LR -  10-CV Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#recall_score = cross_val_score(ANOVA_svm_model, descriptors_fs, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "f1_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(lr_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "mathews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(lr_clf, descriptors, activities, scoring=mathews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_lr1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_lr1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_lr1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_lr1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_lr1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_lr1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_lr1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_lr1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_lr1], ['Sensitivity',sen_lr1], ['Specificity',spec_lr1], ['Balanced Accuracy',ba_lr1],['F1_score',f1_lr1], ['AUC',auc_lr1], ['Precision',prec_lr1], ['MCC',mcc_lr1]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "log_predictions = lr_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != log_predictions[i]]\n",
    "wrong_predictions_lr = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_lr.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted wrong.')\n",
    "compounds_lr_wrong =  pd.DataFrame(wrong_predictions_lr['ROMol'])\n",
    "\n",
    "for index, row in compounds_lr_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Save Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_lr,\"data/BSEP_WP_LR_Compounds.sdf\",properties=list(wrong_predictions_lr.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Logistic Regression -  Evaluation Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_predict = lr_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_lr2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), lr_predict)\n",
    "sen_lr2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), lr_predict)\n",
    "spec_lr2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), lr_predict, pos_label=0)\n",
    "ba_lr2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), lr_predict)\n",
    "f1_lr2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), lr_predict)\n",
    "auc_lr2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), lr_predict))\n",
    "prec_lr2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), lr_predict)\n",
    "mcc_lr2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), lr_predict)\n",
    "rec_lr2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), lr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_lr2], ['Sensitivity',sen_lr2], ['Specificity',spec_lr2], ['Balanced Accuracy',ba_lr2],['F1_score',f1_lr2], ['AUC',auc_lr2], ['Precision',prec_lr2], ['MCC',mcc_lr2], ['Recall',rec_lr2]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test[\"Prediction\"] = lr_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_LR_Test = df_Test\n",
    "df_LR_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_LR_Test,\"BSEP_LR_Test_Prediction.sdf\",properties=list(df_LR_Test.columns))\n",
    "df_LR_Test.to_csv(\"results/BSEP_LR_Test_Prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2) Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC(class_weight='balanced', C= 1.0, kernel= 'linear')\n",
    "svm_clf.fit(descriptors,activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_SVM.pkl'\n",
    "pickle.dump(svm_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### SVM  -  10-CV Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(svm_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "mathews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(svm_clf, descriptors, activities, scoring=mathews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_svm1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_svm1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_svm1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_svm1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_svm1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_svm1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_svm1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_svm1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_svm1], ['Sensitivity',sen_svm1], ['Specificity',spec_svm1], ['Balanced Accuracy',ba_svm1],['F1_score',f1_svm1], ['AUC',auc_svm1], ['Precision',prec_svm1], ['MCC',mcc_svm1]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "svm_predictions = svm_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != svm_predictions[i]]\n",
    "wrong_predictions_svm = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_svm.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted wrong.')\n",
    "compounds_svm_wrong =  pd.DataFrame(wrong_predictions_svm['ROMol'])\n",
    "\n",
    "for index, row in compounds_svm_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Save Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_svm,\"results/BSEP_WP_SVM_Compounds.sdf\",properties=list(wrong_predictions_svm.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  SVM -  Evaluation Test  Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_predict = svm_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_svm2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), svm_predict)\n",
    "sen_svm2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), svm_predict)\n",
    "spec_svm2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), svm_predict, pos_label=0)\n",
    "ba_svm2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), svm_predict)\n",
    "f1_svm2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), svm_predict)\n",
    "auc_svm2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), svm_predict))\n",
    "prec_svm2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), svm_predict)\n",
    "mcc_svm2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), svm_predict)\n",
    "rec_svm2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), svm_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_svm2], ['Sensitivity',sen_svm2], ['Specificity',spec_svm2], ['Balanced Accuracy',ba_svm2],['F1_score',f1_svm2], ['AUC',auc_svm2], ['Precision',prec_svm2], ['MCC',mcc_svm2],['Recall',rec_svm2]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test[\"Prediction\"] = svm_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_SVM_Test = df_Test\n",
    "df_SVM_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_SVM_Test,\"results/BSEP_SVM_Test_Prediction.sdf\",properties=list(df_SVM_Test.columns))\n",
    "df_SVM_Test.to_csv(\"results/BSEP_SVM_Test_Prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3) Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(class_weight=\"balanced\", max_depth = 4, n_estimators = 10)\n",
    "rf_clf.fit(descriptors, activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_RF.pkl'\n",
    "pickle.dump(rf_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### RF - 10-CV Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(rf_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "mathews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(rf_clf, descriptors, activities, scoring=mathews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_rf1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_rf1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_rf1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_rf1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_rf1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_rf1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_rf1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_rf1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_rf1], ['Sensitivity',sen_rf1], ['Specificity',spec_rf1], ['Balanced Accuracy',ba_rf1],['F1_score',f1_rf1], ['AUC',auc_rf1], ['Precision',prec_rf1], ['MCC',mcc_rf1]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "rf_predictions = rf_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != rf_predictions[i]]\n",
    "wrong_predictions_rf = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_rf.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted wrong.')\n",
    "compounds_rf_wrong =  pd.DataFrame(wrong_predictions_rf['ROMol'])\n",
    "\n",
    "for index, row in compounds_rf_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Save Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_rf,\"results/BSEP_WP_RF_Compounds.sdf\",properties=list(wrong_predictions_rf.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### RF - Evaluation Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_predict = rf_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_rf2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), rf_predict)\n",
    "sen_rf2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), rf_predict)\n",
    "spec_rf2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), rf_predict, pos_label=0)\n",
    "ba_rf2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), rf_predict)\n",
    "f1_rf2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), rf_predict)\n",
    "auc_rf2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), rf_predict))\n",
    "prec_rf2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), rf_predict)\n",
    "mcc_rf2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), rf_predict)\n",
    "rec_rf2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), rf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_rf2], ['Sensitivity',sen_rf2], ['Specificity',spec_rf2], ['Balanced Accuracy',ba_rf2],['F1_score',f1_rf2], ['AUC',auc_rf2], ['Precision',prec_rf2], ['MCC',mcc_rf2],['Recall',rec_rf2]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test[\"Prediction\"] = rf_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_RF_Test = df_Test\n",
    "df_RF_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_RF_Test,\"results/BSEP_RF_Test_Prediction.sdf\",properties=list(df_RF_Test.columns))\n",
    "df_RF_Test.to_csv(\"results/BSEP_RF_Test_Prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4) K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(metric =  'euclidean', n_neighbors = 3, weights =  'distance')\n",
    "knn_clf.fit(descriptors,activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'results/BSEP_model_KNN.pkl'\n",
    "pickle.dump(knn_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  KNN -  10-CV Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accuracy_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='accuracy', cv=10, n_jobs=-1, error_score='raise')\n",
    "sensitivity_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='recall', cv=10, n_jobs=-1, error_score='raise')\n",
    "specificity_cv = make_scorer(recall_score, pos_label=0)\n",
    "specificity_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring=specificity_cv, cv=10, n_jobs=-1, error_score='raise')\n",
    "balanced_accuracy_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='balanced_accuracy', cv=10, n_jobs=-1, error_score='raise' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='f1', cv=10, n_jobs=-1, error_score='raise')\n",
    "auc_cv = cross_val_score(knn_clf, descriptors, activities, scoring='roc_auc', cv=10, n_jobs=-1, error_score='raise')\n",
    "precision_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring='precision', cv=10, n_jobs=-1, error_score='raise')\n",
    "mathews_corrcoef_cv = make_scorer(matthews_corrcoef)\n",
    "matthews_corrcoef_score_cv = cross_val_score(knn_clf, descriptors, activities, scoring=mathews_corrcoef_cv, cv=10, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_knn1 = 'Accuracy: %.3f (%.3f)' % (accuracy_score_cv.mean(), accuracy_score_cv.std())\n",
    "sen_knn1 = 'Sensitivity: %.3f (%.3f)' % (sensitivity_score_cv.mean(), sensitivity_score_cv.std())\n",
    "spec_knn1 = 'Specificity: %.3f (%.3f)' % (specificity_score_cv.mean(), specificity_score_cv.std())\n",
    "ba_knn1 = 'Balanced_Accuracy: %.3f (%.3f)' % (balanced_accuracy_score_cv.mean(), balanced_accuracy_score_cv.std())\n",
    "f1_knn1 = 'F1_score %.3f (%.3f)' % (f1_score_cv.mean(), f1_score_cv.std())\n",
    "auc_knn1 = 'AUC %.3f (%.3f)' % (auc_cv.mean(), auc_cv.std())\n",
    "prec_knn1 = 'Precision %.3f (%.3f)' % (precision_score_cv.mean(), precision_score_cv.std())\n",
    "mcc_knn1 = 'Matthews_corrcoef: %.3f (%.3f)' % (matthews_corrcoef_score_cv.mean(), matthews_corrcoef_score_cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_knn1], ['Sensitivity',sen_knn1], ['Specificity',spec_knn1], ['Balanced Accuracy',ba_knn1],['F1_score',f1_knn1], ['AUC',auc_knn1], ['Precision',prec_knn1], ['MCC',mcc_knn1]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "knn_predictions = knn_clf.predict(descriptors_test)\n",
    "indices = [i for i in range(len(activities_test)) if activities_test[i] != knn_predictions[i]]\n",
    "wrong_predictions_knn = df_Test_Data.iloc[indices,:]\n",
    "wrong_predicted_compounds = wrong_predictions_knn.index.tolist()\n",
    "print(str(len((wrong_predicted_compounds))) + ' compounds have been predicted wrong.')\n",
    "compounds_knn_wrong =  pd.DataFrame(wrong_predictions_knn['ROMol'])\n",
    "\n",
    "for index, row in compounds_knn_wrong.iterrows():\n",
    "    display(HTML(str(row['ROMol'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Save Wrong Predicted Compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(wrong_predictions_knn,\"results/BSEP_WP_KNN_Compounds.sdf\",properties=list(wrong_predictions_knn.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  KNN -  Evaluation Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "knn_predict = knn_clf.predict(descriptors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "acc_knn2 = 'Accuracy_score: %.3f' % accuracy_score(np.asarray(activities_test), knn_predict)\n",
    "sen_knn2 = 'Sensitivity: %.3f ' % recall_score(np.asarray(activities_test), knn_predict)\n",
    "spec_knn2 = 'Specificity: %.3f ' % recall_score(np.asarray(activities_test), knn_predict, pos_label=0)\n",
    "ba_knn2 = 'Balanced_accuracy_score: %.3f' % balanced_accuracy_score(np.asarray(activities_test), knn_predict)\n",
    "f1_knn2 = 'F1_score: %.3f' %  f1_score(np.asarray(activities_test), knn_predict)\n",
    "auc_knn2 = 'AUC %.3f' % (roc_auc_score(np.asarray(activities_test), knn_predict))\n",
    "prec_knn2 = 'Precision_score: %.3f' % precision_score(np.asarray(activities_test), knn_predict)\n",
    "mcc_knn2 = 'Matthews_corrcoef: %.3f' % matthews_corrcoef(np.asarray(activities_test), knn_predict)\n",
    "rec_knn2 = 'Recall_score: %.3f' % recall_score(np.asarray(activities_test), knn_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def dropdown(change):\n",
    "    print(change.new)    \n",
    "    \n",
    "Buttons = widgets.Dropdown(\n",
    "    options = [['Accuracy',acc_rf2], ['Sensitivity',sen_rf2], ['Specificity',spec_rf2], ['Balanced Accuracy',ba_rf2],['F1_score',f1_rf2], ['AUC',auc_rf2], ['Precision',prec_rf2], ['MCC',mcc_rf2],['Recall',rec_rf2]],\n",
    "    description= 'Choose one:',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "Buttons.observe(dropdown, names='value')\n",
    "display(Buttons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test[\"Prediction\"] = knn_predict\n",
    "df_Test[\"Prediction Comparison\"] = np.where(pd.to_numeric(df_Test[\"Classification\"]) == df_Test[\"Prediction\"], \"Right\", \"Wrong\")\n",
    "df_Test[\"Domain\"] = df_descriptors_test[\"LOF\"]\n",
    "df_Test[\"Domain\"] = np.where(df_Test.Domain.values == 1, \"In\", \"Out\")\n",
    "df_KNN_Test = df_Test\n",
    "df_KNN_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PandasTools.WriteSDF(df_KNN_Test,\"results/BSEP_RF_Test_Prediction.sdf\",properties=list(df_KNN_Test.columns))\n",
    "df_KNN_Test.to_csv(\"results/BSEP_KNN_Test_Prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
